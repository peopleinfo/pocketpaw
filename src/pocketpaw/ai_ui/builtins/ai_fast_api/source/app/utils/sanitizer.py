"""Response and input sanitizer for G4F proxy.

G4F providers frequently inject ads, tracking URLs, and promotional
content into LLM responses.  This module strips that garbage so
downstream consumers receive clean output.
"""

import re

# ---------------------------------------------------------------------------
# Known spam / ad patterns injected by G4F providers
# ---------------------------------------------------------------------------

# Exact domains and URL patterns known to be injected ads
_AD_DOMAINS: set[str] = {
    "op.wtf",
    "gptgo.ai",
    "freegptsnav.aifree.site",
    "you.com",
    "chatgptfree.ai",
    "gptonline.ai",
    "chat.g4f.run",
    "g4f.ai",
    "aitianhu.com",
    "aitianhu.space",
    "chatforai.store",
    "chathub.gg",
    "gptchatly.com",
    "bettergpt.chat",
    "freechatgpt.chat",
    "talkai.info",
    "poe.com",
    "aichatfree.info",
    "geminiprochat.com",
}

# Promotional phrases that always indicate injected spam
_AD_PHRASES: list[str] = [
    "need proxies cheaper than the market",
    "cheaper than the market",
    "get proxies at",
    "buy proxies at",
    "sponsored by",
    "powered by g4f",
    "powered by gpt4free",
    "visit us at",
    "try it free at",
    "this response was generated by",
    "generated by gpt4free",
    "provided by g4f",
]

# Regex: full line that is just a URL (with optional markdown link syntax)
_URL_LINE_RE = re.compile(
    r"^\s*"
    r"(?:\[.*?\]\()?"  # optional markdown link opening [text](
    r"https?://\S+"
    r"(?:\))?"  # optional markdown link closing )
    r"\s*$",
    re.IGNORECASE,
)

# Regex: URLs containing known ad domains
_AD_URL_RE = re.compile(
    r"https?://(?:www\.)?"
    + "|".join(re.escape(d) for d in _AD_DOMAINS)
    + r"\S*",
    re.IGNORECASE,
)

# Pre-compiled lowercase ad phrases for fast matching
_AD_PHRASES_LOWER = [p.lower() for p in _AD_PHRASES]


def sanitize_response(text: str) -> str:
    """Remove injected ads, spam URLs, and promotional content from an LLM response."""
    if not text:
        return text

    lines = text.split("\n")
    cleaned: list[str] = []
    skip_remaining = False

    for line in lines:
        stripped = line.strip()
        lower = stripped.lower()

        if skip_remaining:
            if stripped:
                continue
            skip_remaining = False
            continue

        # Drop lines that are just a known ad URL
        if _AD_URL_RE.search(stripped):
            skip_remaining = True
            continue

        # Drop lines matching known ad phrases
        if any(phrase in lower for phrase in _AD_PHRASES_LOWER):
            skip_remaining = True
            continue

        # Drop standalone URL lines that appear after the actual content
        # (only if we already have real content â€” preserves URLs in actual answers)
        if cleaned and _URL_LINE_RE.match(stripped) and not _has_context(cleaned, stripped):
            continue

        cleaned.append(line)

    result = "\n".join(cleaned).rstrip()

    # Strip trailing blank lines left after ad removal
    while result.endswith("\n\n"):
        result = result[:-1]

    return result


def sanitize_stream_chunk(text: str) -> str:
    """Light sanitization for individual streaming chunks.

    Full line-based sanitization isn't possible on fragments, so we
    only strip known ad URLs and phrases that appear within a chunk.
    """
    if not text:
        return text

    result = _AD_URL_RE.sub("", text)

    lower = result.lower()
    for phrase in _AD_PHRASES_LOWER:
        idx = lower.find(phrase)
        if idx != -1:
            result = result[:idx]
            lower = result.lower()

    return result


def sanitize_input(text: str) -> str:
    """Sanitize user/system input to limit prompt injection surface.

    Strips control characters, null bytes, and excessively repeated
    whitespace that could be used to hide injected instructions.
    Preserves normal content and formatting.
    """
    if not text:
        return text

    # Strip null bytes and other ASCII control chars (except newline/tab)
    text = re.sub(r"[\x00-\x08\x0b\x0c\x0e-\x1f\x7f]", "", text)

    # Collapse runs of 3+ blank lines into 2 (prevents hiding content in whitespace)
    text = re.sub(r"\n{4,}", "\n\n\n", text)

    # Strip zero-width characters used to hide text
    text = re.sub(r"[\u200b\u200c\u200d\u2060\ufeff]", "", text)

    return text.strip()


def _has_context(previous_lines: list[str], url_line: str) -> bool:
    """Check if a URL line is contextually part of the answer.

    Returns True if the preceding line references the URL or looks like
    an intentional citation (e.g. "See: https://...").
    """
    if not previous_lines:
        return False

    prev = previous_lines[-1].strip().lower()
    link_indicators = ("see:", "source:", "reference:", "link:", "url:", "here:", "check")
    return any(prev.endswith(ind) or prev.endswith(ind + " ") for ind in link_indicators)
