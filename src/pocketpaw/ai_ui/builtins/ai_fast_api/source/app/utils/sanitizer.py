"""Response and input sanitizer for G4F proxy.

- **Input sanitization**: Redacts credit cards, API keys, passwords so they
  are never sent to G4F/external providers.  Default: secure (redact on).
- **Response sanitization**: Strips injected ads, tracking URLs, promotional
  content from LLM output.
- **Config**: SANITIZER_REDACT_INPUT, SANITIZER_EXCLUDE_DOMAINS,
  SANITIZER_EXCLUDE_PHRASES via env.  Defaults remain safe.
"""

import re

# ---------------------------------------------------------------------------
# Sensitive input patterns (redact before sending to G4F)
# ---------------------------------------------------------------------------

# Credit card: 13â€“19 digits, optional spaces/dashes between groups
_CARD_RE = re.compile(
    r"\b(?:\d[-\s]*){12,18}\d\b"
)

# API keys: sk-xxx, Bearer xxx, xai-xxx, etc.
_API_KEY_RE = re.compile(
    r"\b(?:sk|pk|xai|api[_-]?key)[-_]?[a-zA-Z0-9]{20,}\b|"
    r"\bBearer\s+[a-zA-Z0-9_\-\.]{20,}\b|"
    r"\b(?:api[_-]?key|apikey)\s*[:=]\s*['\"]?[a-zA-Z0-9_\-]{20,}['\"]?\b",
    re.IGNORECASE,
)

# Passwords: password=xxx, passwd=xxx, pwd=xxx (4+ chars)
_PASSWORD_RE = re.compile(
    r"\b(?:password|passwd|pwd|secret|token)\s*[:=]\s*['\"]?[^\s'\"\n]{4,}['\"]?\b",
    re.IGNORECASE,
)

# ---------------------------------------------------------------------------
# Default ad domains and phrases (can be extended via config)
# ---------------------------------------------------------------------------

_DEFAULT_AD_DOMAINS: set[str] = {
    "op.wtf",
    "api.airforce",
    "discord.gg/airforce",
    "gptgo.ai",
    "freegptsnav.aifree.site",
    "you.com",
    "chatgptfree.ai",
    "gptonline.ai",
    "chat.g4f.run",
    "g4f.ai",
    "aitianhu.com",
    "aitianhu.space",
    "chatforai.store",
    "chathub.gg",
    "gptchatly.com",
    "bettergpt.chat",
    "freechatgpt.chat",
    "talkai.info",
    "poe.com",
    "aichatfree.info",
    "geminiprochat.com",
    "airforce.chat",
    "chatgpt4online.org",
    "freegpt4.ddns.net",
}

_DEFAULT_AD_PHRASES: list[str] = [
    "need proxies cheaper",
    "need proxies cheaper than the market",
    "cheaper than the market",
    "upgrade your plan to remove this message",
    "upgrade to remove this message",
    "remove this message at",
    "get proxies at",
    "buy proxies at",
    "sponsored by",
    "powered by g4f",
    "powered by gpt4free",
    "powered by airforce",
    "visit us at",
    "try it free at",
    "this response was generated by",
    "generated by gpt4free",
    "provided by g4f",
    "provided by airforce",
    "join our discord",
    "discord.gg/airforce",
]


def _get_ad_domains() -> set[str]:
    """Return ad domains: defaults + config extras."""
    try:
        from ..config import settings

        extra = [
            d.strip().lower()
            for d in settings.sanitizer_exclude_domains.split(",")
            if d.strip()
        ]
        return _DEFAULT_AD_DOMAINS | set(extra)
    except Exception:
        return _DEFAULT_AD_DOMAINS


def _get_ad_phrases() -> list[str]:
    """Return ad phrases: defaults + config extras."""
    try:
        from ..config import settings

        extra = [
            p.strip().lower()
            for p in settings.sanitizer_exclude_phrases.split(",")
            if p.strip()
        ]
        return _DEFAULT_AD_PHRASES + extra
    except Exception:
        return _DEFAULT_AD_PHRASES


def _build_patterns() -> tuple[set[str], list[str], re.Pattern]:
    domains = _get_ad_domains()
    phrases = _get_ad_phrases()
    ad_url_re = re.compile(
        r"(?:https?://)?(?:www\.)?"
        + "(?:" + "|".join(re.escape(d) for d in domains) + ")"
        + r"\S*",
        re.IGNORECASE,
    )
    return domains, phrases, ad_url_re


_URL_LINE_RE = re.compile(
    r"^\s*"
    r"(?:\[.*?\]\()?"
    r"https?://\S+"
    r"(?:\))?"
    r"\s*$",
    re.IGNORECASE,
)


# ---------------------------------------------------------------------------
# Input redaction (cards, API keys, passwords)
# ---------------------------------------------------------------------------


def redact_sensitive(text: str) -> str:
    """Redact credit cards, API keys, passwords from user input.

    Prevents accidental leakage to G4F and external providers.
    Controlled by SANITIZER_REDACT_INPUT (default: true).
    """
    if not text:
        return text

    try:
        from ..config import settings

        if not settings.sanitizer_redact_input:
            return text
    except Exception:
        pass

    result = _CARD_RE.sub("[REDACTED_CARD]", text)
    result = _API_KEY_RE.sub("[REDACTED_API_KEY]", result)
    result = _PASSWORD_RE.sub("[REDACTED_SECRET]", result)

    return result


# ---------------------------------------------------------------------------
# Response sanitization (ads)
# ---------------------------------------------------------------------------


def sanitize_response(text: str) -> str:
    """Remove injected ads, spam URLs, promotional content from LLM response."""
    if not text:
        return text

    domains, phrases, ad_url_re = _build_patterns()
    domains_lower = {d.lower() for d in domains}
    phrases_lower = [p.lower() for p in phrases]

    lines = text.split("\n")
    cleaned: list[str] = []
    skip_remaining = False

    for line in lines:
        stripped = line.strip()
        lower = stripped.lower()

        if skip_remaining:
            if stripped:
                continue
            skip_remaining = False
            continue

        if ad_url_re.search(stripped):
            skip_remaining = True
            continue

        if any(d in lower for d in domains_lower):
            skip_remaining = True
            continue

        if any(phrase in lower for phrase in phrases_lower):
            skip_remaining = True
            continue

        if cleaned and _URL_LINE_RE.match(stripped) and not _has_context(cleaned, stripped):
            continue

        cleaned.append(line)

    result = "\n".join(cleaned).rstrip()

    while result.endswith("\n\n"):
        result = result[:-1]

    return result


class StreamSanitizer:
    """Stateful sanitizer for character-by-character streaming.

    Buffers each line until a newline, then checks against ad patterns.
    """

    def __init__(self):
        self._line_buf = ""
        self._poisoned = False

    def feed(self, chunk: str) -> str:
        """Feed a chunk (may be a single char) and return safe-to-emit text."""
        if not chunk:
            return ""

        domains, phrases, ad_url_re = _build_patterns()
        domains_lower = {d.lower() for d in domains}
        phrases_lower = [p.lower() for p in phrases]

        result_parts: list[str] = []

        for ch in chunk:
            if ch == "\n":
                if self._poisoned:
                    self._line_buf = ""
                    continue

                clean = self._check_line(
                    self._line_buf, domains_lower, phrases_lower, ad_url_re
                )
                if clean is not None:
                    result_parts.append(clean)
                    result_parts.append("\n")
                else:
                    self._poisoned = True

                self._line_buf = ""
            else:
                self._line_buf += ch

        return "".join(result_parts)

    def flush(self) -> str:
        """Flush remaining buffer at end of stream."""
        if self._poisoned:
            self._line_buf = ""
            self._poisoned = False
            return ""

        if not self._line_buf:
            return ""

        domains, phrases, ad_url_re = _build_patterns()
        domains_lower = {d.lower() for d in domains}
        phrases_lower = [p.lower() for p in phrases]

        clean = self._check_line(
            self._line_buf, domains_lower, phrases_lower, ad_url_re
        )
        self._line_buf = ""
        return clean if clean is not None else ""

    @staticmethod
    def _check_line(
        line: str,
        domains_lower: set[str],
        phrases_lower: list[str],
        ad_url_re: re.Pattern,
    ) -> str | None:
        stripped = line.strip()
        if not stripped:
            return line

        lower = stripped.lower()

        if ad_url_re.search(stripped):
            return None

        if any(d in lower for d in domains_lower):
            return None

        if any(phrase in lower for phrase in phrases_lower):
            return None

        return line


def sanitize_stream_chunk(text: str) -> str:
    """Light sanitization for multi-token chunks.  Use StreamSanitizer for char-by-char."""
    if not text:
        return text

    _, _, ad_url_re = _build_patterns()
    domains_lower = {d.lower() for d in _get_ad_domains()}
    phrases_lower = [p.lower() for p in _get_ad_phrases()]

    result = ad_url_re.sub("", text)
    lower = result.lower()

    for d in domains_lower:
        idx = lower.find(d)
        if idx != -1:
            result = result[:idx]
            lower = result.lower()

    for phrase in phrases_lower:
        idx = lower.find(phrase)
        if idx != -1:
            result = result[:idx]
            lower = result.lower()

    return result


def sanitize_input(text: str) -> str:
    """Sanitize user/system input before sending to G4F.

    1. Redacts credit cards, API keys, passwords (if SANITIZER_REDACT_INPUT=true)
    2. Strips control chars, zero-width chars, excessive whitespace
    """
    if not text:
        return text

    text = redact_sensitive(text)

    text = re.sub(r"[\x00-\x08\x0b\x0c\x0e-\x1f\x7f]", "", text)
    text = re.sub(r"\n{4,}", "\n\n\n", text)
    text = re.sub(r"[\u200b\u200c\u200d\u2060\ufeff]", "", text)

    return text.strip()


def _has_context(previous_lines: list[str], url_line: str) -> bool:
    if not previous_lines:
        return False

    prev = previous_lines[-1].strip().lower()
    indicators = ("see:", "source:", "reference:", "link:", "url:", "here:", "check")
    return any(prev.endswith(ind) or prev.endswith(ind + " ") for ind in indicators)
